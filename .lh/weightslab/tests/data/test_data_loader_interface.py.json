{
    "sourceFile": "weightslab/tests/data/test_data_loader_interface.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1771952353032,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1771952353032,
            "name": "Commit-0",
            "content": "import math\r\nimport unittest\r\nimport torch\r\nfrom torch.utils.data import TensorDataset, DataLoader, Subset, Dataset, get_worker_info\r\nfrom torchvision import datasets, transforms\r\n\r\nfrom weightslab.utils.tools import capture_rng_state, restore_rng_state, seed_everything\r\nfrom weightslab.backend.dataloader_interface import DataLoaderInterface\r\nfrom weightslab.components.global_monitoring import pause_controller\r\nfrom weightslab.backend import ledgers\r\n\r\n\r\ndef infinite_loader(loader):\r\n    \"\"\"Generator that yields batches indefinitely, restarting the loader each epoch.\r\n\r\n    This respects `shuffle` semantics of the wrapped DataLoader because a new\r\n    iterator is created at the start of each epoch.\r\n    \"\"\"\r\n    while True:\r\n        for batch in loader:\r\n            yield batch\r\n\r\n\r\nclass WorkerIdDataset(Dataset):\r\n    \"\"\"Dataset that returns the worker ID for testing multi-worker functionality.\"\"\"\r\n    def __init__(self, size: int):\r\n        self.size = size\r\n\r\n    def __len__(self):\r\n        return self.size\r\n\r\n    def __getitem__(self, idx: int):\r\n        info = get_worker_info()\r\n        worker_id = info.id if info is not None else -1\r\n        data = torch.tensor([idx], dtype=torch.long)\r\n        target = torch.tensor(worker_id, dtype=torch.long)\r\n        return data, target\r\n\r\n\r\nclass TestDataLoaderInterface(unittest.TestCase):\r\n    def setUp(self):\r\n        # Ensure controller is in resumed state for test\r\n        pause_controller._resume()\r\n\r\n        # small dataset sizes to keep tests fast\r\n        self.train_size = 100\r\n        self.test_size = 40\r\n        self.batch_size = 8\r\n\r\n        def make_dataset(size):\r\n            data = torch.randn(size, 1, 28, 28)\r\n            # give each sample a unique label so uniqueness checks match dataset size\r\n            labels = torch.arange(size, dtype=torch.long)\r\n            return TensorDataset(data, labels)\r\n\r\n        train_ds = make_dataset(self.train_size)\r\n        test_ds = make_dataset(self.test_size)\r\n\r\n        self.train_ds = train_ds\r\n        self.test_ds = test_ds\r\n\r\n        self.train_loader = DataLoader(train_ds, batch_size=self.batch_size, shuffle=False)\r\n        self.test_loader = DataLoader(test_ds, batch_size=self.batch_size, shuffle=False)\r\n\r\n    def _consume_batches_collect_labels(self, loader, max_batches=None):\r\n        \"\"\"Consume up to max_batches (or whole epoch if None) and return list of labels seen.\"\"\"\r\n        labels = []\r\n        for i, batch in enumerate(loader):\r\n            labels.extend(batch[1].tolist())\r\n            if max_batches is not None and i + 1 >= max_batches:\r\n                break\r\n        return labels\r\n\r\n    def test_iteration_covers_entire_dataset(self):\r\n        # iterate a full epoch and collect unique labels\r\n        labels = self._consume_batches_collect_labels(self.train_loader)\r\n        dataset_size = len(self.train_loader.dataset)\r\n        # number of unique labels seen should equal dataset size (labels are unique per sample here)\r\n        self.assertEqual(len(set(labels)), dataset_size)\r\n\r\n        # index of last batch should be len(loader) - 1\r\n        expected_batches = math.ceil(dataset_size / self.train_loader.batch_size)\r\n        self.assertEqual(len(self.train_loader), expected_batches)\r\n\r\n    def test_iterator_next_raises_stopiteration_after_epoch(self):\r\n        it = iter(self.train_loader)\r\n        # consume exactly one epoch\r\n        for _ in range(len(self.train_loader)):\r\n            next(it)\r\n\r\n        # next call should raise StopIteration\r\n        with self.assertRaises(StopIteration):\r\n            next(it)\r\n\r\n    def test_dataloader_interface_worker_defaults_and_override(self):\r\n        iface_default = DataLoaderInterface(self.train_ds, batch_size=self.batch_size)\r\n        self.assertEqual(iface_default.dataloader.num_workers, 0)\r\n        self.assertTrue(iface_default.dataloader.pin_memory)\r\n\r\n        iface_override = DataLoaderInterface(\r\n            self.train_ds,\r\n            batch_size=self.batch_size,\r\n            num_workers=2,\r\n            pin_memory=False,\r\n        )\r\n        self.assertEqual(iface_override.dataloader.num_workers, 2)\r\n        self.assertFalse(iface_override.dataloader.pin_memory)\r\n\r\n    def test_dataloader_interface_uses_multiple_workers(self):\r\n        dataset = WorkerIdDataset(64)\r\n \r\n        train_iface = DataLoaderInterface(\r\n            dataset,\r\n            batch_size=1,\r\n            shuffle=False,\r\n            num_workers=2,\r\n            pin_memory=False,\r\n        )\r\n\r\n        test_iface = DataLoaderInterface(\r\n            dataset,\r\n            batch_size=1,\r\n            shuffle=False,\r\n            num_workers=2,\r\n            pin_memory=False,\r\n        )\r\n\r\n        def _collect_worker_ids(iface, max_batches=32):\r\n            worker_ids = set()\r\n            for i, batch in enumerate(iface):\r\n                worker_tensor = batch[-1]\r\n                worker_ids.add(int(worker_tensor.reshape(-1)[0].item()))\r\n                if i + 1 >= max_batches:\r\n                    break \r\n            return worker_ids\r\n\r\n        train_worker_ids = _collect_worker_ids(train_iface)\r\n        test_worker_ids = _collect_worker_ids(test_iface)\r\n\r\n        self.assertGreaterEqual(len(train_worker_ids), 2)\r\n        self.assertGreaterEqual(len(test_worker_ids), 2)\r\n\r\n    def test_infinite_loader_restarts_epochs_and_collects_all_labels(self):\r\n        inf = infinite_loader(self.train_loader)\r\n        labels = []\r\n        num_calls = 300\r\n        for _ in range(num_calls):\r\n            batch = next(inf)\r\n            labels.extend(batch[1].tolist())\r\n\r\n        # after many calls we should still have seen every sample at least once\r\n        self.assertEqual(len(set(labels)), len(self.train_loader.dataset))\r\n\r\n\r\nclass TestDataLoaderReproducibility(unittest.TestCase):\r\n    \"\"\"Test RNG and iteration state reproducibility for dataloaders.\"\"\"\r\n\r\n    @classmethod\r\n    def setUpClass(cls):\r\n        \"\"\"Set up test dataset once for all reproducibility tests.\"\"\"\r\n        # Use a small MNIST subset for testing\r\n        transform = transforms.Compose([\r\n            transforms.ToTensor(),\r\n            transforms.Normalize((0.1307,), (0.3081,))\r\n        ])\r\n\r\n        # Auto register\r\n        hp = ledgers.get_hyperparams()\r\n        hp['ledger_flush_interval'] = 10  # Disable flushing threads for tests\r\n        hp['ledger_flush_max_rows'] = 15  # Disable flushing threads for tests\r\n        hp['ledger_enable_h5_persistence'] = False  # Disable flushing threads for tests\r\n        hp['ledger_enable_flushing_threads'] = False  # Disable flushing threads for tests\r\n\r\n        # Set controller to resumed state\r\n        pause_controller._resume()\r\n\r\n        try:\r\n            # Try to load from common location\r\n            full_dataset = datasets.MNIST(\r\n                root='C:/Users/GuillaumePelluet/Desktop/mnist_data/',\r\n                train=False,\r\n                download=False,\r\n                transform=transform\r\n            )\r\n        except:\r\n            # Fallback to temp directory\r\n            import tempfile\r\n            temp_dir = tempfile.mkdtemp()\r\n            full_dataset = datasets.MNIST(\r\n                root=temp_dir,\r\n                train=False,\r\n                download=True,\r\n                transform=transform\r\n            )\r\n\r\n        # Create subset with 100 samples\r\n        subset_indices = list(range(100))\r\n        cls.dataset = Subset(full_dataset, subset_indices)\r\n\r\n    def test_rng_reproducibility_with_shuffle(self):\r\n        \"\"\"Test dataloader reproducibility with shuffle: save RNG → generate batches → reload RNG → verify same batches.\r\n\r\n        Key insight: Shuffle happens when iter() is called. Restoring RNG before\r\n        reset_iterator() ensures identical shuffle ordering.\r\n        \"\"\"\r\n        print(f\"\\n{'='*60}\")\r\n        print(\"RNG State Reproducibility - Shuffle Enabled\")\r\n        print(f\"{'='*60}\\n\")\r\n\r\n        # 1. Initialize with seed and create dataloader\r\n        print(\"1. Initializing with seed=42...\")\r\n        seed_everything(42)\r\n\r\n        dataloader = DataLoaderInterface(\r\n            self.dataset,\r\n            batch_size=2,\r\n            shuffle=True,\r\n            num_workers=0\r\n        )\r\n        print(f\"[OK] DataLoader created (batch_size=2, shuffle=True)\")\r\n\r\n        # Consume initial batches\r\n        next(dataloader)\r\n        next(dataloader)\r\n\r\n        # 2. Capture RNG state\r\n        print(\"\\n2. Capturing RNG state...\")\r\n        rng_state = capture_rng_state()\r\n        dataloader.reset_iterator()  # Reset to use captured RNG\r\n        print(f\"[OK] RNG state captured and iterator reset\")\r\n\r\n        # 3. Generate batches with current RNG\r\n        print(\"\\n3. Generating batches...\")\r\n        _, bids_1, _ = next(dataloader)\r\n        _, bids_2, _ = next(dataloader)\r\n        print(f\"Batches: {bids_1.tolist()}, {bids_2.tolist()}\")\r\n\r\n        # 4. Restore RNG and reset iterator\r\n        print(\"\\n4. Restoring RNG state and resetting iterator...\")\r\n        restore_rng_state(rng_state)\r\n        dataloader.reset_iterator()\r\n        print(f\"[OK] RNG restored, iterator reset\")\r\n\r\n        # 5. Generate batches again - should be identical\r\n        print(\"\\n5. Generating batches with restored RNG...\")\r\n        _, bids_1_repeat, _ = next(dataloader)\r\n        _, bids_2_repeat, _ = next(dataloader)\r\n        print(f\"Repeated batches: {bids_1_repeat.tolist()}, {bids_2_repeat.tolist()}\")\r\n\r\n        # Verify\r\n        print(f\"\\n{'='*60}\")\r\n        print(\"Verification:\")\r\n        print(f\"  Batch 1 match: {torch.equal(bids_1, bids_1_repeat)}\")\r\n        print(f\"  Batch 2 match: {torch.equal(bids_2, bids_2_repeat)}\")\r\n        self.assertTrue(torch.equal(bids_1, bids_1_repeat), \"First batches should be identical\")\r\n        self.assertTrue(torch.equal(bids_2, bids_2_repeat), \"Second batches should be identical\")\r\n        print(f\"[OK] RNG reproducibility verified!\\n\")\r\n\r\n    # TODO (GP): Re-enable once OffsetSampler is implemented and tested\r\n    # def test_iteration_state_reproducibility_without_shuffle(self):\r\n    #     \"\"\"Test dataloader reproducibility without shuffle: capture iteration state → resume identically.\r\n\r\n    #     With shuffle disabled, RNG is irrelevant. We capture the iteration position\r\n    #     (number of batches yielded) and restore that position efficiently using\r\n    #     OffsetSampler to skip samples at the index level without data reprocessing.\r\n    #     \"\"\"\r\n    #     print(f\"\\n{'='*60}\")\r\n    #     print(\"Iteration State Reproducibility - No Shuffle\")\r\n    #     print(f\"{'='*60}\\n\")\r\n\r\n    #     print(\"1. Creating dataloader (shuffle=False)...\")\r\n    #     dataloader = DataLoaderInterface(\r\n    #         self.dataset,\r\n    #         batch_size=2,\r\n    #         shuffle=False,\r\n    #         num_workers=0\r\n    #     )\r\n    #     print(f\"[OK] DataLoader created (batch_size=2, shuffle=False)\")\r\n\r\n    #     # 2. Consume two batches, then capture state\r\n    #     print(\"\\n2. Consuming first 2 batches...\")\r\n    #     _, bids_1, _ = next(dataloader)\r\n    #     _, bids_2, _ = next(dataloader)\r\n    #     print(f\"Batches 1-2: {bids_1.tolist()}, {bids_2.tolist()}\")\r\n\r\n    #     iter_state = dataloader.capture_iteration_state()\r\n    #     print(f\"[OK] Iteration state captured: {iter_state}\")\r\n\r\n    #     # 3. Consume next two batches\r\n    #     print(\"\\n3. Consuming batches 3-4...\")\r\n    #     _, bids_3, _ = next(dataloader)\r\n    #     _, bids_4, _ = next(dataloader)\r\n    #     print(f\"Batches 3-4: {bids_3.tolist()}, {bids_4.tolist()}\")\r\n\r\n    #     # 4. Restore iteration state\r\n    #     print(f\"\\n4. Restoring to position after batch 2...\")\r\n    #     dataloader.restore_iteration_state(iter_state)\r\n    #     print(f\"[OK] Iteration state restored (skipped first 2 batches efficiently)\")\r\n\r\n    #     # 5. Generate batches again - should match 3 and 4\r\n    #     print(\"\\n5. Generating next batches (should match 3-4)...\")\r\n    #     _, bids_3_repeat, _ = next(dataloader)\r\n    #     _, bids_4_repeat, _ = next(dataloader)\r\n    #     print(f\"Repeated batches: {bids_3_repeat.tolist()}, {bids_4_repeat.tolist()}\")\r\n\r\n    #     # Verify\r\n    #     print(f\"\\n{'='*60}\")\r\n    #     print(\"Verification:\")\r\n    #     print(f\"  Batch 3 match: {torch.equal(bids_3, bids_3_repeat)}\")\r\n    #     print(f\"  Batch 4 match: {torch.equal(bids_4, bids_4_repeat)}\")\r\n    #     self.assertTrue(torch.equal(bids_3, bids_3_repeat), \"Batch 3 should be identical\")\r\n    #     self.assertTrue(torch.equal(bids_4, bids_4_repeat), \"Batch 4 should be identical\")\r\n    #     print(f\"[OK] Iteration state reproducibility verified!\\n\")\r\n\r\n\r\n"
        }
    ]
}