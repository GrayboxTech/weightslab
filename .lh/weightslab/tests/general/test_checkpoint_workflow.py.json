{
    "sourceFile": "weightslab/tests/general/test_checkpoint_workflow.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1771952180880,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1771952180880,
            "name": "Commit-0",
            "content": "\"\"\"\r\nComprehensive Unit Tests for Checkpoint System V3\r\n\r\nTests the complete workflow of experiment checkpointing with:\r\n- Model architecture changes\r\n- Hyperparameter updates\r\n- Data state changes (tags, discard)\r\n- Checkpoint reloading and branching\r\n\r\nTests are separated into individual methods (init, testA, B, C, D, E)\r\nwith state preserved in class variables between tests.\r\n\"\"\"\r\n\r\nimport os\r\nimport random\r\nimport unittest\r\nimport tempfile\r\nimport warnings\r\nimport json\r\nimport pandas as pd\r\n\r\nwarnings.filterwarnings(\"ignore\")\r\n\r\nimport weightslab as wl\r\nimport torch as th\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nfrom torch.utils.data import Subset\r\nfrom torchvision import datasets, transforms\r\nfrom tqdm import trange\r\nfrom pathlib import Path\r\n\r\n# Import components directly to avoid full weightslab initialization\r\nfrom weightslab.components.checkpoint_manager import CheckpointManager\r\nfrom weightslab.components.experiment_hash import ExperimentHashGenerator\r\nfrom weightslab.data.sample_stats import SampleStatsEx\r\nfrom weightslab.utils.logger import LoggerQueue\r\nfrom weightslab.backend import ledgers\r\nfrom weightslab.components.global_monitoring import (\r\n    guard_training_context,\r\n    start_hp_sync_thread_event,\r\n    pause_controller\r\n)\r\nfrom weightslab.utils.tools import seed_everything\r\n\r\n\r\n# Init hp sync thread event (used in training loop to trigger HP checkpointing)\r\nstart_hp_sync_thread_event()\r\n\r\n\r\n# Helper function to register objects in ledger directly\r\ndef register_in_ledger(obj, flag, device='cpu', **kwargs):\r\n    \"\"\"Register an object in the ledger.\"\"\"\r\n    try:\r\n        if flag == \"hyperparameters\":\r\n            return wl.watch_or_edit(\r\n                obj,\r\n                flag=\"hyperparameters\",\r\n                defaults=obj,\r\n                poll_interval=1.0,\r\n                **kwargs\r\n            )\r\n        elif flag == \"model\":\r\n            return wl.watch_or_edit(\r\n                obj,\r\n                flag=\"model\",\r\n                device=device,\r\n                compute_dependencies=True,\r\n                **kwargs\r\n            )\r\n        elif flag == \"dataloader\":\r\n           return wl.watch_or_edit(\r\n                obj,\r\n                flag=\"data\",\r\n                **kwargs\r\n            )\r\n        elif flag == \"optimizer\":\r\n            return wl.watch_or_edit(\r\n                obj,\r\n                flag=\"optimizer\",\r\n                **kwargs\r\n            )\r\n        elif flag == \"signal\":\r\n            return wl.watch_or_edit(\r\n                obj,\r\n                flag=\"signal\",\r\n                **kwargs\r\n            )\r\n    except Exception:\r\n        # If direct registration fails, silently continue\r\n        pass\r\n\r\n\r\n# Set seed for reproducibility\r\nseed_everything()\r\nDEVICE = \"cuda\" if th.cuda.is_available() else \"cpu\"\r\nEXP_NAME = \"mnist_checkpoint_test_v3\"\r\n\r\n\r\nclass SimpleCNN(nn.Module):\r\n    \"\"\"Simple CNN for MNIST classification\"\"\"\r\n\r\n    def __init__(self, conv1_out=8, conv2_out=16):\r\n        super(SimpleCNN, self).__init__()\r\n        self.input_shape = (1, 1, 28, 28)  # MNIST input shape\r\n        self.conv1 = nn.Conv2d(1, conv1_out, kernel_size=3, padding=1)\r\n        self.pool1 = nn.MaxPool2d(2, 2)\r\n        self.conv2 = nn.Conv2d(conv1_out, conv2_out, kernel_size=3, padding=1)\r\n        self.pool2 = nn.MaxPool2d(2, 2)\r\n        self.fc1 = nn.Linear(conv2_out * 7 * 7, 64)\r\n        self.fc2 = nn.Linear(64, 10)\r\n\r\n    def forward(self, x):\r\n        x = self.pool1(F.relu(self.conv1(x)))\r\n        x = self.pool2(F.relu(self.conv2(x)))\r\n        x = x.view(x.size(0), -1)\r\n        x = F.relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n\r\n\r\nclass TaggableDataset:\r\n    \"\"\"Wrapper for dataset with tagging and discard functionality\"\"\"\r\n\r\n    def __init__(self, dataset):\r\n        self.dataset = dataset\r\n        self._tags = {}\r\n        self._discarded = set()\r\n\r\n    def __len__(self):\r\n        return len(self.dataset)\r\n\r\n    def __getitem__(self, idx):\r\n        def _pack_item(sample_idx):\r\n            data, target = self.dataset[sample_idx]\r\n            uid = th.tensor(sample_idx, dtype=th.long)\r\n            return data, uid, target\r\n\r\n        if idx in self._discarded:\r\n            # Return next non-discarded sample\r\n            for i in range(idx + 1, len(self.dataset)):\r\n                if i not in self._discarded:\r\n                    return _pack_item(i)\r\n            # Wrap around\r\n            for i in range(0, idx):\r\n                if i not in self._discarded:\r\n                    return _pack_item(i)\r\n        return _pack_item(idx)\r\n\r\n    def get_sample_uids(self):\r\n        \"\"\"Return list of sample UIDs\"\"\"\r\n        return [i for i in range(len(self.dataset))]\r\n\r\n    def is_discarded(self, uid):\r\n        \"\"\"Check if sample is discarded\"\"\"\r\n        idx = str(uid.split('_')[1])\r\n        return idx in self._discarded\r\n\r\n    def discard(self, uid):\r\n        \"\"\"Discard a sample\"\"\"\r\n        idx = str(uid.split('_')[1])\r\n        self._discarded.add(idx)\r\n\r\n    def add_tag(self, uid, tag):\r\n        \"\"\"Add tag to sample\"\"\"\r\n        if uid not in self._tags:\r\n            self._tags[uid] = []\r\n        if tag not in self._tags[uid]:\r\n            self._tags[uid].append(tag)\r\n\r\n    def get_tags(self, uid):\r\n        \"\"\"Get tags for sample\"\"\"\r\n        return self._tags.get(uid, [])\r\n\r\n    def get_data_state(self):\r\n        \"\"\"Get complete data state for checkpointing\"\"\"\r\n        uids = self.get_sample_uids()\r\n        return {\r\n            'uids': uids,\r\n            'discarded': self._discarded.copy(),\r\n            'tags': self._tags.copy()\r\n        }\r\n\r\n\r\nclass CheckpointSystemTests(unittest.TestCase):\r\n    \"\"\"Comprehensive tests for checkpoint system with separated test methods\"\"\"\r\n\r\n    # Class variables to preserve state across tests\r\n    temp_dir = None\r\n    log_dir = None\r\n    dataset = None\r\n    config = None\r\n    manager = None\r\n\r\n    # State tracking for each experiment\r\n    state = {\r\n        'exp_hash_a': None,\r\n        'exp_hash_b': None,\r\n        'exp_hash_c': None,\r\n        'exp_hash_d': None,\r\n        'exp_hash_e': None,\r\n        'exp_hash_f': None,\r\n        'exp_hash_g': None,\r\n        'exp_hash_h': None,\r\n        'exp_hash_i': None,\r\n        'exp_hash_j': None,\r\n        'exp_hash_k': None,\r\n        'losses_a': None,\r\n        'losses_b': None,\r\n        'losses_c': None,\r\n        'losses_d': None,\r\n        'losses_e': None,\r\n        'losses_f': None,\r\n        'losses_g': None,\r\n        'losses_h': None,\r\n        'losses_i': None,\r\n        'losses_j': None,\r\n        'losses_k': None,\r\n    }\r\n\r\n\r\n    def train_epochs(self, model, loader, optimizer, criterion, num_epochs, criterion_bin=None):\r\n        \"\"\"Train model for specified epochs with checkpointing\"\"\"\r\n        losses = []\r\n        uids_trained = []\r\n        for _ in trange(num_epochs, desc=\"Training\"):\r\n            with guard_training_context:\r\n                epoch_loss = 0.0\r\n                batch_count = 0\r\n\r\n                # Data Processing\r\n                (inputs, ids, labels) = next(loader)\r\n                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\r\n                uids_trained.extend(ids)\r\n\r\n                # Inference\r\n                optimizer.zero_grad()\r\n                preds_raw = model(inputs)\r\n\r\n                # Preds\r\n                if preds_raw.ndim == 1:\r\n                    preds = (preds_raw > 0.0).long()\r\n                else:\r\n                    preds = preds_raw.argmax(dim=1, keepdim=True)\r\n\r\n                # Losses\r\n                # # Binary loss\r\n                if criterion_bin is not None:\r\n                    loss = criterion_bin(\r\n                        preds_raw[:, 7],\r\n                        (labels==7).float(),\r\n                        batch_ids=ids,\r\n                        preds=preds\r\n                    )\r\n                # Loss and backward\r\n                loss = criterion(\r\n                    preds_raw,\r\n                    labels,\r\n                    batch_ids=ids,\r\n                    preds=preds\r\n                )\r\n                loss.mean().backward()\r\n                optimizer.step()\r\n                epoch_loss += loss.mean().item()\r\n                batch_count += 1\r\n\r\n                avg_loss = epoch_loss / batch_count if batch_count > 0 else 0\r\n                losses.append(avg_loss)\r\n        print(f\"Trained on {uids_trained}.\")\r\n        return losses, uids_trained\r\n\r\n    def check_reproducibility(self, original_loss, reloaded_loss, original_uids=None, reloaded_uids=None, loss_tol=0.1, uids_msg=None):\r\n        \"\"\"Common reproducibility check for losses and UIDs\"\"\"\r\n        return \r\n        # #   Check reproducibility of losses and UIDs\r\n        # if isinstance(original_loss, (list, tuple)):\r\n        #     original_loss_sum = sum(original_loss)/len(original_loss)\r\n        # else:\r\n        #     original_loss_sum = original_loss\r\n        # if isinstance(reloaded_loss, (list, tuple)):\r\n        #     reloaded_loss_sum = sum(reloaded_loss)/len(reloaded_loss)\r\n        # else:\r\n        #     reloaded_loss_sum = reloaded_loss\r\n        # loss_diff = abs(original_loss_sum - reloaded_loss_sum)\r\n        # loss_relative_diff = loss_diff / original_loss_sum if original_loss_sum != 0 else 0\r\n        # print(f\"[OK] Loss comparison:\")\r\n        # print(f\"  Original: {original_loss_sum:.6f}\")\r\n        # print(f\"  Reloaded: {reloaded_loss_sum:.6f}\")\r\n        # print(f\"  Relative difference: {loss_relative_diff*100:.3f}%\")\r\n        # self.assertLess(loss_relative_diff, loss_tol, msg=f\"Training should be reproducible within {loss_tol*100:.1f}%\")\r\n        # if original_uids is not None and reloaded_uids is not None:\r\n        #     print(f\"[OK] UIDs comparison:\")\r\n        #     print(f\"  Original: {original_uids}\")\r\n        #     print(f\"  Reloaded: {reloaded_uids}\")\r\n        #     self.assertListEqual(reloaded_uids, original_uids, msg=uids_msg or \"Sample UIDs should match for reproducibility\")\r\n\r\n    @classmethod\r\n    def setUpClass(cls):\r\n        \"\"\"Set up test environment once before all tests\"\"\"\r\n        print(\"\\n\" + \"=\"*80)\r\n        print(\"CHECKPOINT SYSTEM V3 - COMPREHENSIVE TESTS (SEPARATED)\")\r\n        print(\"=\"*80 + \"\\n\")\r\n\r\n        # Init pause controller\r\n        pause_controller.pause()\r\n\r\n        # Create temporary directory (used for all tests)\r\n        cls.temp_dir = tempfile.mkdtemp(prefix=\"checkpoint_v3_test_\")\r\n        cls.log_dir = os.path.join(cls.temp_dir, \"experiments\")\r\n\r\n        # Initialize config from YAML-like dict (similar to ws-classification)\r\n        cls.config = {\r\n            'experiment_name': EXP_NAME,\r\n            'device': DEVICE,\r\n            'root_log_dir': cls.log_dir,\r\n\r\n            # Data parameters\r\n            'data': {\r\n                'train_loader': {\r\n                    'batch_size': 2,\r\n                    'shuffle': False\r\n                },\r\n            },\r\n\r\n            'experiment_dump_to_train_steps_ratio': 5,\r\n            'skip_checkpoint_load': False,\r\n\r\n            # Configure global dataframe storage\r\n            'ledger_enable_flushing_threads': True,\r\n            'ledger_enable_h5_persistence': True,\r\n            'ledger_flush_max_rows': 4,\r\n            'ledger_flush_interval': 5.0,\r\n\r\n            # Configure clients\r\n            'serving_grpc': False,\r\n            'serving_cli': False,\r\n\r\n            # Training parameters\r\n            'training': {\r\n                'num_epochs': 11,\r\n            },\r\n\r\n            # Optimizer parameters\r\n            'optimizer': {\r\n                'lr': 0.001\r\n            }\r\n        }\r\n        cls.config_cp = cls.config.copy()\r\n\r\n        # ==================\r\n        # Initialize dataset\r\n        # ==================\r\n        # Load MNIST subset (10 samples for all tests)\r\n        transform = transforms.Compose([\r\n            transforms.ToTensor(),\r\n            transforms.Normalize((0.1307,), (0.3081,))\r\n        ])\r\n        full_dataset = datasets.MNIST(\r\n            # root=os.path.join(cls.temp_dir, 'data'),\r\n            root='C:/Users/GuillaumePelluet/Desktop/mnist_data/',\r\n            train=False,\r\n            download=True,\r\n            transform=transform\r\n        )\r\n        mnist_subset = Subset(full_dataset, list(range(10)))  # Create subset with 10 samples\r\n        cls.dataset = TaggableDataset(mnist_subset)  # Wrap in taggable dataset\r\n\r\n        # =================\r\n        # Initialize Logger\r\n        # =================\r\n        cls.logger = LoggerQueue(register=True)\r\n\r\n        # =============\r\n        # Initialize HP\r\n        # =============\r\n        # Register HP in ledger\r\n        cls.config = register_in_ledger(cls.config, flag=\"hyperparameters\")\r\n\r\n        # ================\r\n        # Initialize Model\r\n        # ================\r\n        model = SimpleCNN(conv1_out=8, conv2_out=16)\r\n        model = register_in_ledger(model, flag=\"model\", device=DEVICE, skip_previous_auto_load=True)\r\n\r\n        # =====================\r\n        # Initialize DataLoader\r\n        # =====================\r\n        register_in_ledger(\r\n            cls.dataset,\r\n            flag=\"dataloader\",\r\n            compute_hash=False,\r\n            is_training=True,\r\n            batch_size=cls.config.get('data', {}).get('train_loader', {}).get('batch_size', 32),\r\n            shuffle=cls.config.get('data', {}).get('train_loader', {}).get('shuffle', False)\r\n        )\r\n\r\n        # ==================================\r\n        # Initialize Criterion and Optimizer\r\n        # ==================================\r\n        # Optimizer and criterion\r\n        # # Create and register optimizer\r\n        register_in_ledger(\r\n            th.optim.Adam(model.parameters(),\r\n            lr=cls.config.get('optimizer', {}).get('lr', 0.001)),\r\n            flag=\"optimizer\"\r\n        )\r\n        # # Create and register signal (criterion)\r\n        register_in_ledger(\r\n            nn.CrossEntropyLoss(reduction='none'),\r\n            flag=\"signal\",\r\n            log=True,\r\n            name=\"train_mlt_loss/CE\"\r\n        )\r\n        register_in_ledger(\r\n            nn.BCEWithLogitsLoss(reduction='none'),\r\n            flag=\"signal\",\r\n            log=True,\r\n            name=\"train_bin_loss/BCE\"\r\n        )\r\n\r\n        # =================================\r\n        # Get the global checkpoint manager\r\n        # =================================\r\n        cls.chkpt_manager = ledgers.get_checkpoint_manager()\r\n\r\n        # ============================\r\n        # Print setup info\r\n        print(f\"[OK] Created MNIST subset: {len(cls.dataset)} samples\")\r\n        print(f\"[OK] Temporary directory: {cls.temp_dir}\")\r\n        print(f\"[OK] Config initialized\")\r\n        print(f\"[OK] Checkpoint manager initialized at {cls.config.get('root_log_dir')}\\n\")\r\n\r\n    # ==============================\r\n    # Test: 00_initialize_experiment\r\n    # ==============================\r\n    def test_00_initialize_experiment(self):\r\n        \"\"\"Initialize experiment with configuration and first model\"\"\"\r\n        print(f\"\\n{'='*80}\")\r\n        print(\"TEST 00: Initialize Experiment Configuration\")\r\n        print(f\"{'='*80}\\n\")\r\n\r\n        # Initialize hyperparameters with model_age\r\n        exp_hash_a, _, changed = self.chkpt_manager.update_experiment_hash(firsttime=True)\r\n\r\n        print(f\"\\n[OK] Experiment hash A: {exp_hash_a}\")\r\n        print(f\"[OK] Changed components: {changed}\")\r\n\r\n        self.assertTrue(os.path.exists(self.chkpt_manager.models_dir))\r\n        self.assertTrue(os.path.exists(self.chkpt_manager.hp_dir))\r\n        self.assertTrue(os.path.exists(self.chkpt_manager.data_checkpoint_dir))\r\n        self.assertTrue(os.path.exists(self.chkpt_manager.manifest_file))\r\n\r\n        # Store in state for next tests\r\n        self.state['exp_hash_a'] = exp_hash_a\r\n\r\n        print(f\"\\n[OK] TEST 00 PASSED - Experiment initialized\")\r\n\r\n    # ================\r\n    # Test: 01_train_A\r\n    # ================\r\n    def test_01_train_A(self):\r\n        \"\"\"Train initial model for 11 epochs\"\"\"\r\n        print(f\"\\n{'='*80}\")\r\n        print(\"TEST A: Initialize and First Training\")\r\n        print(f\"{'='*80}\\n\")\r\n\r\n        # Get stored state from previous test and load it\r\n        exp_hash_a = self.state['exp_hash_a']\r\n        success = self.chkpt_manager.load_state(exp_hash=exp_hash_a)\r\n        self.assertTrue(success, \"Checkpoint load should succeed\")\r\n\r\n        # Model\r\n        model = ledgers.get_model()\r\n\r\n        # Dataloader\r\n        dataloader = ledgers.get_dataloader()\r\n\r\n        # Optimizer and criterion\r\n        optimizer = ledgers.get_optimizer()\r\n        criterion = ledgers.get_signal(name=\"train_mlt_loss/CE\")\r\n        criterion_bin = ledgers.get_signal(name=\"train_bin_loss/BCE\")\r\n\r\n        # Training\r\n        print(\"Training for 11 epochs with checkpoint frequency 5...\")\r\n        pause_controller.resume()\r\n        loss_A, uids_A = self.train_epochs(\r\n            model, dataloader, optimizer, criterion,\r\n            num_epochs=self.config['training']['num_epochs'],\r\n            criterion_bin=criterion_bin\r\n        )\r\n        pause_controller.pause()\r\n        print(\"\\nTraining completed.\")\r\n\r\n        # Verify checkpoints\r\n        model_dir_a = self.chkpt_manager.models_dir / exp_hash_a[8:-8]\r\n        self.assertTrue(model_dir_a.exists(), \"Model checkpoint directory should exist\")\r\n\r\n        # Check for weight checkpoints\r\n        weight_files = list(model_dir_a.glob(\"*_step_*.pt\"))\r\n        print(f\"[OK] Found {len(weight_files)} weight checkpoint files\")\r\n        self.assertGreaterEqual(len(weight_files), 2, \"Should have at least 2 weight checkpoints\")\r\n\r\n        # Check HP directory\r\n        hp_dir_a = self.chkpt_manager.hp_dir / exp_hash_a[:8]\r\n        self.assertTrue(hp_dir_a.exists(), \"HP checkpoint directory should exist\")\r\n\r\n        # Check data directory\r\n        data_dir_a = self.chkpt_manager.data_checkpoint_dir / exp_hash_a[-8:]\r\n        self.assertTrue(data_dir_a.exists(), \"Data checkpoint directory should exist\")\r\n\r\n        # Save state for next tests\r\n        self.state['exp_hash_a'] = exp_hash_a\r\n        self.state['losses_a'] = sum(loss_A) / len(loss_A)\r\n        self.state['uids_a'] = uids_A\r\n\r\n        # Final verbose\r\n        print(f\"  Final model_age (i.e., how many epochs lived by the model): {model.current_step}\")\r\n        print(f\"\\n[OK] TEST A PASSED - Initial training completed\")\r\n\r\n    # =============================\r\n    # Test: 02_train_B_model_change\r\n    # =============================\r\n    def test_02_train_B_model_change(self):\r\n        \"\"\"Modify model architecture and train for 11 epochs\"\"\"\r\n        print(f\"\\n{'='*80}\")\r\n        print(\"TEST B: Modify Model Architecture\")\r\n        print(f\"{'='*80}\\n\")\r\n\r\n        # Model\r\n        model = ledgers.get_model()\r\n\r\n        # Dataloader\r\n        dataloader = ledgers.get_dataloader()\r\n\r\n        # Optimizer and criterion\r\n        optimizer = ledgers.get_optimizer()\r\n        criterion = ledgers.get_signal(name=\"train_mlt_loss/CE\")\r\n        criterion_bin = ledgers.get_signal(name=\"train_bin_loss/BCE\")\r\n\r\n        print(\"Modifying model architecture...\")\r\n\r\n        # Modify model architecture\r\n        # model.operate(0, {-1, -2, -3, -4}, 1)  # Increase conv1 out channels by 2\r\n        # model.operate(2, {-1}, 2)  # Freeze fc1 layer\r\n        model.operate(-2, {}, 3)  # Freeze fc1 layer\r\n        model.operate(-1, {1}, 4)  # Reset fc2 layer\r\n\r\n        print(f\"  Conv1: 8 -> 12 channels\")\r\n        print(f\"  Conv2: 16 -> 15 channels\")\r\n        print(f\"  FC1: Frozen\")\r\n        print(f\"  FC2: Reset\")\r\n\r\n        # Update hash here to get hash\r\n        exp_hash_b, _, changed = self.chkpt_manager.update_experiment_hash()\r\n        print(f\"\\n[OK] New experiment hash B: {exp_hash_b}\")\r\n        print(f\"[OK] Changed components: {changed}\")\r\n        self.assertIn('model', changed, \"Model should have changed\")\r\n        self.assertNotEqual(self.state['exp_hash_a'], exp_hash_b, \"Hash should be different\")\r\n\r\n        print(\"\\nResuming training for 11 epochs...\")\r\n        pause_controller.resume()\r\n        loss_B, uids_B = self.train_epochs(\r\n            model, dataloader, optimizer, criterion,\r\n            num_epochs=self.config['training']['num_epochs'],\r\n            criterion_bin=criterion_bin\r\n        )\r\n        pause_controller.pause()\r\n        print(\"\\nTraining completed.\")\r\n\r\n        # Verify new model directory\r\n        model_dir_b = self.chkpt_manager.models_dir / exp_hash_b[8:-8]\r\n        self.assertTrue(model_dir_b.exists(), \"New model checkpoint directory should exist\")\r\n        weight_files_b = list(model_dir_b.glob(\"*_step_*.pt\"))\r\n        print(f\"[OK] Found {len(weight_files_b)} weight checkpoint files in new directory\")\r\n        self.assertGreaterEqual(len(weight_files_b), 2, \"Should have at least 2 new weight checkpoints\")\r\n\r\n        # Store state\r\n        self.state['exp_hash_b'] = exp_hash_b\r\n        self.state['losses_b'] = sum(loss_B) / len(loss_B)\r\n        self.state['uids_b'] = uids_B\r\n\r\n        # Final verbose\r\n        print(f\"\\n[OK] TEST B PASSED - Model architecture updated\")\r\n        print(f\"  Final model_age: {model.current_step}\")\r\n\r\n    # ========================================================================\r\n    # Test: 03_train_C_hyperparams_change\r\n    # ========================================================================\r\n    def test_03_train_C_hyperparams_change(self):\r\n        \"\"\"Change hyperparameters and train for 11 epochs\"\"\"\r\n        print(f\"\\n{'='*80}\")\r\n        print(\"TEST C: Change Hyperparameters\")\r\n        print(f\"{'='*80}\\n\")\r\n\r\n        # Model\r\n        model = ledgers.get_model()\r\n\r\n        # Dataloader\r\n        dataloader = ledgers.get_dataloader()\r\n\r\n        # Optimizer and criterion\r\n        optimizer = ledgers.get_optimizer()\r\n        criterion = ledgers.get_signal(name=\"train_mlt_loss/CE\")\r\n        criterion_bin = ledgers.get_signal(name=\"train_bin_loss/BCE\")\r\n\r\n        print(\"Changing hyperparameters...\")\r\n\r\n        # Change batch size\r\n        new_bs = 3\r\n        self.config['data']['train_loader']['batch_size'] = new_bs\r\n        print(f\"  Batch size: 2 -> 4\")\r\n\r\n        # Update hash\r\n        exp_hash_c, _, _ = self.chkpt_manager.update_experiment_hash()\r\n\r\n        print(f\"\\n[OK] New experiment hash C: {exp_hash_c}\")\r\n        self.assertNotEqual(self.state['exp_hash_b'], exp_hash_c, \"Hash should be different as hp changed\")\r\n\r\n        print(\"\\nResuming training for 11 epochs...\")\r\n        pause_controller.resume()\r\n        loss_C, uids_C = self.train_epochs(\r\n            model, dataloader, optimizer, criterion,\r\n            num_epochs=self.config['training']['num_epochs'],\r\n            criterion_bin=criterion_bin\r\n        )\r\n        pause_controller.pause()\r\n\r\n        print(\"\\nTraining completed.\")\r\n\r\n        # Verify new HP directory\r\n        hp_dir_c = self.chkpt_manager.hp_dir / exp_hash_c[:8]\r\n        self.assertTrue(hp_dir_c.exists(), \"New HP checkpoint directory should exist\")\r\n\r\n        # Verify model weights still being saved\r\n        model_dir_c = self.chkpt_manager.models_dir / exp_hash_c[8:-8]\r\n        weight_files_c = list(model_dir_c.glob(\"*_step_*.pt\"))\r\n        print(f\"[OK] Found {len(weight_files_c)} weight checkpoint files\")\r\n        self.assertGreaterEqual(len(weight_files_c), 2, \"Should have at least 2 weight checkpoints\")\r\n\r\n        # Store state\r\n        self.state['exp_hash_c'] = exp_hash_c\r\n        self.state['losses_c'] = sum(loss_C) / len(loss_C)\r\n        self.state['uids_c'] = uids_C\r\n        self.state['new_bs_C'] = self.config['data']['train_loader']['batch_size']\r\n\r\n        # Final verbose\r\n        print(f\"\\n[OK] TEST C PASSED - Hyperparameters updated\")\r\n        print(f\"  Final model_age (i.e., how many epochs lived by the model): {model.current_step}\")\r\n\r\n    # ========================================================================\r\n    # Test: 04_train_D_data_change\r\n    # ========================================================================\r\n    def test_04_train_D_data_change(self):\r\n        \"\"\"Change data state (tags and discard) and train for 11 epochs\"\"\"\r\n        print(f\"\\n{'='*80}\")\r\n        print(\"TEST D: Change Data State (Tags and Discard)\")\r\n        print(f\"{'='*80}\\n\")\r\n\r\n        # Model\r\n        model = ledgers.get_model()\r\n\r\n        # Data\r\n        dataloader = ledgers.get_dataloader()  # Get dataloader\r\n        dfm = ledgers.get_dataframe()  # Get dataframe manager\r\n\r\n        # Optimizer and criterion\r\n        optimizer = ledgers.get_optimizer()\r\n        criterion = ledgers.get_signal(name=\"train_mlt_loss/CE\")\r\n        criterion_bin = ledgers.get_signal(name=\"train_bin_loss/BCE\")\r\n\r\n        print(\"Modifying data...\")\r\n\r\n        # Add 20 random tags with 'ugly'\r\n        tagged_samples = random.sample(range(10), 4)\r\n        rows = []\r\n        uids_discarded = []\r\n        for idx in tagged_samples:\r\n            uid = dfm.get_df_view().index[idx]\r\n            uids_discarded.append(uid)\r\n            rows.append(\r\n                {\r\n                    \"sample_id\": uid,\r\n                    f\"{SampleStatsEx.TAG.value}:ugly\": True,  # Random tag with 'ugly'\r\n                    SampleStatsEx.DISCARDED.value: bool(1 - dfm.get_df_view()[SampleStatsEx.DISCARDED.value].iloc[idx])\r\n                }\r\n            )\r\n\r\n        # Updates data - Simulate adding tags and discarding samples in dataset\r\n        df_update = pd.DataFrame(rows).set_index(\"sample_id\")\r\n        # upsert_df updates the ledger's dataframe immediately\r\n        dfm.upsert_df(df_update, origin='train_loader', force_flush=True)\r\n\r\n        # Changes will be pending\r\n        print(f\"  Added 'ugly' tag to 20 samples\")\r\n        print(f\"  Discarded 20 samples\")\r\n\r\n        # Update hash\r\n        exp_hash_d, _, changed = self.chkpt_manager.update_experiment_hash()\r\n\r\n        print(f\"\\n[OK] New experiment hash D: {exp_hash_d}\")\r\n        print(f\"[OK] Changed components: {changed}\")\r\n        self.assertIn('data', changed, \"Data should have changed\")\r\n        self.assertNotEqual(self.state['exp_hash_c'], exp_hash_d, \"Hash should be different\")\r\n\r\n        print(\"\\nResuming training for 11 epochs...\")\r\n        pause_controller.resume()  # Pending changes to dump: data state\r\n        loss_D, uids_D = self.train_epochs(\r\n            model, dataloader, optimizer, criterion,\r\n            num_epochs=self.config['training']['num_epochs'],\r\n            criterion_bin=criterion_bin\r\n        )\r\n        pause_controller.pause()\r\n\r\n        print(\"\\nTraining completed.\")\r\n\r\n        # Verify new data directory\r\n        data_dir_d = self.chkpt_manager.data_checkpoint_dir / exp_hash_d[-8:]\r\n        self.assertTrue(data_dir_d.exists(), \"New data checkpoint directory should exist\")\r\n\r\n        # Verify model weights still being saved\r\n        model_dir_d = self.chkpt_manager.models_dir / exp_hash_d[8:-8]\r\n        weight_files_d = list(model_dir_d.glob(\"*_step_*.pt\"))\r\n        print(f\"[OK] Found {len(weight_files_d)} weight checkpoint files\")\r\n        self.assertGreaterEqual(len(weight_files_d), 2, \"Should have at least 2 weight checkpoints\")\r\n\r\n        # Store state\r\n        self.state['exp_hash_d'] = exp_hash_d\r\n        self.state['losses_d'] = sum(loss_D) / len(loss_D)\r\n        self.state['uids_d'] = uids_D\r\n        self.state['uids_discarded_d'] = uids_discarded\r\n        self.state['model_c1_neurons'] = model.layers[0].out_neurons\r\n\r\n        # Final verbose\r\n        print(f\"\\n[OK] TEST D PASSED - Data state updated\")\r\n        print(f\"  Final model_age (i.e., how many epochs lived by the model): {model.current_step}\")\r\n\r\n    # ========================================================================\r\n    # Test: 05_train_E_reload_and_branch\r\n    # ========================================================================\r\n    def test_05_train_E_reload_and_branch(self):\r\n        \"\"\"Reload state B and branch with modified HP and data\"\"\"\r\n        print(f\"\\n{'='*80}\")\r\n        print(\"TEST E: Reload State B and Branch\")\r\n        print(f\"{'='*80}\\n\")\r\n\r\n        # Get hp from original training\r\n        hp_original = self.config\r\n\r\n        print(\"Experiment paused. Analyzing experiment history...\")\r\n\r\n        # Get all hashes\r\n        all_hashes = self.chkpt_manager.get_all_hashes(sort_by='created')\r\n        print(f\"\\n[OK] Found {len(all_hashes)} experiment states:\")\r\n        for i, entry in enumerate(all_hashes):\r\n            print(f\"  {i+1}. {entry['hash'][:16]}... (created: {entry['created'][:19]})\")\r\n\r\n        # Reload state B (second state created)\r\n        hash_a_from_manifest = self.state['exp_hash_a']\r\n\r\n        print(f\"\\n[OK] Reloading state B: {hash_a_from_manifest[:16]}...\")\r\n\r\n        # Use new load_state method to load and apply checkpoint in-place\r\n        success = self.chkpt_manager.load_state(exp_hash=hash_a_from_manifest)\r\n        self.assertTrue(success, \"State should be loaded successfully\")\r\n\r\n        # Get components from ledger (they were updated in-place)\r\n        model_reloaded = ledgers.get_model()\r\n        hp_reloaded = ledgers.get_hyperparams()\r\n\r\n        print(f\"[OK] State applied successfully\")\r\n        print(f\"[OK] Loaded HP: {hp_reloaded}\")\r\n\r\n        # Modify HP and data\r\n        print(\"\\nModifying HP and data (not training yet)...\")\r\n\r\n        # Handle nested config structure\r\n        if 'data' in hp_reloaded and 'train_loader' in hp_reloaded['data']:\r\n            hp_reloaded['data']['train_loader']['batch_size'] = 1\r\n            old_batch_size = hp_original.get('data', {}).get('train_loader', {}).get('batch_size', 2)\r\n            print(f\"  Batch size: {old_batch_size} -> 1\")\r\n\r\n        # Discard more data\r\n        # Add 20 random tags with 'ugly'\r\n        tagged_samples = random.sample(range(10), 1)\r\n        rows = []\r\n        dfm = ledgers.get_dataframe()  # Get dataframe manager\r\n        for idx in tagged_samples:\r\n            uid = dfm.get_df_view().index[idx]\r\n            rows.append(\r\n                {\r\n                    \"sample_id\": uid,\r\n                    f\"{SampleStatsEx.TAG.value}:ugly\": True,\r\n                    SampleStatsEx.DISCARDED.value: bool(1 - dfm.get_df_view(SampleStatsEx.DISCARDED.value).iloc[idx])\r\n                }\r\n            )\r\n        # # # Updates data - Simulate adding tags and discarding samples in dataset\r\n        # # # upsert_df updates the ledger's dataframe immediately\r\n        dfm.upsert_df(pd.DataFrame(rows).set_index(\"sample_id\"), origin='train_loader', force_flush=True)\r\n\r\n        # Update hash with all changes\r\n        exp_hash_e, _, changed = self.chkpt_manager.update_experiment_hash()\r\n\r\n        print(f\"\\n[OK] New experiment hash E (branch): {exp_hash_e}\")\r\n        print(f\"[OK] Changed components: {changed}\")\r\n        self.assertIn('hp', changed, \"HP should have changed\")\r\n        self.assertIn('data', changed, \"Data should have changed\")\r\n\r\n        # Update ledger\r\n        # Ledger is already registered as proxy are used\r\n        pass\r\n\r\n        # Setting training environment from loader\r\n        dataloader = ledgers.get_dataloader()\r\n        model = ledgers.get_model()\r\n        optimizer = ledgers.get_optimizer()\r\n        criterion = ledgers.get_signal(name=\"train_mlt_loss/CE\")\r\n        criterion_bin = ledgers.get_signal(name=\"train_bin_loss/BCE\")\r\n\r\n        print(\"\\nResuming training for 21 epochs...\")\r\n        pause_controller.resume()\r\n        loss_E, uids_E = self.train_epochs(\r\n            model_reloaded, dataloader, optimizer, criterion, criterion_bin=criterion_bin,\r\n            num_epochs=self.config['training']['num_epochs'] * 2,\r\n        )\r\n        pause_controller.pause()\r\n\r\n        print(\"\\nTraining completed.\")\r\n\r\n        # Verify checkpoints for E\r\n        model_dir_e = self.chkpt_manager.models_dir / exp_hash_e[8:-8]\r\n        weight_files_e = list(model_dir_e.glob(\"*_step_*.pt\"))\r\n        print(f\"[OK] Found {len(weight_files_e)} weight checkpoint files\")\r\n        self.assertGreaterEqual(len(weight_files_e), 4, \"Should have at least 4 weight checkpoints for 21 epochs\")\r\n\r\n        # Store state\r\n        self.state['exp_hash_e'] = exp_hash_e\r\n        self.state['losses_e'] = loss_E\r\n        self.state['uids_e'] = uids_E\r\n\r\n        print(f\"\\n[OK] TEST E PASSED - Reloaded and generate a new train branch successfully\")\r\n        print(f\"  Final model_age: {model.current_step}\")\r\n\r\n    # ========================================================================\r\n    # Test: 06_reload_before_model_change\r\n    # ========================================================================\r\n    def test_06_reload_before_model_change(self):\r\n        \"\"\"Reload before model change (back to A), fix conv size with RNG replay, verify HP+data\"\"\"\r\n        print(f\"\\n{'='*80}\")\r\n        print(\"TEST 06: Reload Before Model Change - Fix Conv Size with RNG State\")\r\n        print(f\"{'='*80}\\n\")\r\n\r\n        hash_A_original = self.state['exp_hash_a']  # Before model change\r\n        loss_A_original = self.state['losses_a']  # Before model change\r\n        uids_A_original = self.state['uids_a']  # Before model change\r\n\r\n        print(f\"Reloading state A (before model change) for verification: {hash_A_original[:16]}...\")\r\n        success = self.chkpt_manager.load_state(exp_hash=hash_A_original)\r\n        self.assertTrue(success, \"State A should load successfully\")\r\n\r\n        # Verify HP and data are from checkpoint A\r\n        hp_reloaded = ledgers.get_hyperparams()\r\n\r\n        print(f\"[OK] HP batch_size: {hp_reloaded.get('data', {}).get('train_loader', {}).get('batch_size', 'N/A')}\")\r\n        self.assertEqual(hp_reloaded.get('data', {}).get('train_loader', {}).get('batch_size'), 2,\r\n                        \"Should have batch_size=2 from state A\")\r\n        print(f\"[OK] Data state verified from state A\")\r\n        print(f\"[OK] RNG state restored for reproducible batching\")\r\n\r\n        # Train with original model to verify batches are the same\r\n        print(\"\\nTraining with original model from state A (11 epochs)...\")\r\n        model_original = ledgers.get_model()\r\n        dataloader_original = ledgers.get_dataloader()\r\n        optimizer_original = ledgers.get_optimizer()\r\n        criterion = ledgers.get_signal(name=\"train_mlt_loss/CE\")\r\n        criterion_bin = ledgers.get_signal(name=\"train_bin_loss/BCE\")\r\n\r\n        pause_controller.resume()\r\n        loss_A_reloaded, uids_A_reloaded = self.train_epochs(\r\n            model_original, dataloader_original, optimizer_original, criterion,\r\n            num_epochs=self.config['training']['num_epochs'],\r\n            criterion_bin=criterion_bin\r\n        )\r\n        pause_controller.pause()\r\n\r\n        # Check reproducibility with original loss and UIDs\r\n        self.check_reproducibility(loss_A_original, loss_A_reloaded, uids_A_original, uids_A_reloaded)\r\n\r\n        # Reload again and fix model, should get same batches due to restored RNG\r\n        print(f\"\\nReloading state A again (to reset RNG for fair comparison) and modifying model architecture...\")\r\n        success = self.chkpt_manager.load_state(exp_hash=hash_A_original)\r\n        self.assertTrue(success, \"State A should load successfully second time\")\r\n\r\n        # Fix model conv size - create new model with different architecture\r\n        print(\"\\nFixing model architecture...\")\r\n        model = ledgers.get_model()\r\n        model.operate(0, {-1}, 1)\r\n        model.operate(2, {-1}, 2)\r\n        model.operate(-2, {}, 3)\r\n        model.operate(-1, {-1 }, 4)\r\n\r\n        exp_hash_h, _, changed = self.chkpt_manager.update_experiment_hash()\r\n        print(f\"\\n[OK] New experiment hash H: {exp_hash_h[:16]}\")\r\n        print(f\"[OK] Changed components: {changed}\")\r\n        self.assertIn('model', changed, \"Only model should have changed\")\r\n        self.assertNotIn('hp', changed, \"HP should not have changed\")\r\n        self.assertNotIn('data', changed, \"Data should not have changed\")\r\n\r\n        # Train with new model - should get same batches due to restored RNG\r\n        dataloader = ledgers.get_dataloader()\r\n        optimizer = ledgers.get_optimizer()\r\n        criterion = ledgers.get_signal(name=\"train_mlt_loss/CE\")\r\n        criterion_bin = ledgers.get_signal(name=\"train_bin_loss/BCE\")\r\n\r\n        print(\"\\nTraining for 11 epochs with new model (same RNG state = same batches)...\")\r\n        pause_controller.resume()\r\n        loss_H, uids_H = self.train_epochs(model, dataloader, optimizer, criterion, num_epochs=self.config['training']['num_epochs'],\r\n            criterion_bin=criterion_bin)\r\n        pause_controller.pause()\r\n\r\n        print(f\"[OK] Fixed model training loss (first/last): {loss_H} / {loss_H}\")\r\n\r\n        # Compare: First batch should be same, but losses differ due to different model\r\n        print(f\"\\n[OK] Reproducibility verified:\")\r\n        print(f\"  Original model first batch loss: {loss_A_reloaded}\")\r\n        print(f\"  Fixed model first batch loss: {loss_H}\")\r\n        print(f\"  (Same RNG = same batches, different losses due to model change)\")\r\n\r\n        # Store state\r\n        self.state['losses_h'] = loss_H\r\n        self.state['exp_hash_h'] = exp_hash_h\r\n        self.state['uids_h'] = uids_H\r\n\r\n        print(f\"\\n[OK] TEST 06 PASSED - Reloaded with RNG state, trained with fixed architecture\")\r\n\r\n    # ========================================================================\r\n    # Test: 07_change_data_from_test06\r\n    # ========================================================================\r\n    def test_07_change_data_from_test06(self):\r\n        \"\"\"Change data from test 06 - discard more data and train again\"\"\"\r\n        print(f\"\\n{'='*80}\")\r\n        print(\"TEST 07: Change Data from Test 06 - Discard More Data\")\r\n        print(f\"{'='*80}\\n\")\r\n\r\n        hash_H = self.state['exp_hash_h']  # From test 06\r\n\r\n        print(f\"Starting from state H: {hash_H[:16]}...\")\r\n\r\n        # Discard additional 15 samples (total 25% discarded)\r\n        print(\"\\nDiscarding additional 15 samples (25% total)...\")\r\n        dfm = ledgers.get_dataframe()\r\n        tagged_samples = random.sample(range(10), 2)\r\n        rows = []\r\n        for idx in tagged_samples:\r\n            uid = dfm.get_df_view().index[idx]\r\n            rows.append({\r\n                \"sample_id\": uid,\r\n                f\"{SampleStatsEx.TAG.value}:discard_25pct\": True,\r\n                SampleStatsEx.DISCARDED.value: True\r\n            })\r\n\r\n        df_update = pd.DataFrame(rows).set_index(\"sample_id\")\r\n        dfm.upsert_df(df_update, origin='train_loader', force_flush=True)\r\n\r\n        exp_hash_i, _, changed = self.chkpt_manager.update_experiment_hash()\r\n        print(f\"\\n[OK] New experiment hash I: {exp_hash_i[:16]}\")\r\n        print(f\"[OK] Changed components: {changed}\")\r\n        self.assertIn('data', changed, \"Only data should have changed\")\r\n\r\n        # Train for 11 epochs\r\n        model = ledgers.get_model()\r\n        dataloader = ledgers.get_dataloader()\r\n        optimizer = ledgers.get_optimizer()\r\n        criterion = ledgers.get_signal(name=\"train_mlt_loss/CE\")\r\n        criterion_bin = ledgers.get_signal(name=\"train_bin_loss/BCE\")\r\n\r\n        print(\"\\nTraining for 11 epochs with 25% discarded...\")\r\n        pause_controller.resume()\r\n        loss_I, uids_I = self.train_epochs(model, dataloader, optimizer, criterion, num_epochs=self.config['training']['num_epochs'],\r\n            criterion_bin=criterion_bin)\r\n        pause_controller.pause()\r\n\r\n        # Store state\r\n        self.state['losses_i'] = loss_I\r\n        self.state['uids_i'] = uids_I\r\n        self.state['exp_hash_i'] = exp_hash_i\r\n\r\n        print(f\"\\n[OK] TEST 07 PASSED - Changed data and trained successfully\")\r\n\r\n    # ========================================================================\r\n    # Test: 08_reload_before_data_change_verify_and_modify\r\n    # ========================================================================\r\n    def test_08_reload_before_data_change_verify_and_modify(self):\r\n        \"\"\"Reload before data change (state C), verify training reproducibility, then modify model\"\"\"\r\n        print(f\"\\n{'='*80}\")\r\n        print(\"TEST 08: Reload Before Data Change - Verify and Modify Model\")\r\n        print(f\"{'='*80}\\n\")\r\n\r\n        hash_c = self.state['exp_hash_c']  # Before data change (after HP change)\r\n\r\n        print(f\"Part A: Reloading state C and verifying training reproducibility...\")\r\n        print(f\"Reloading state C: {hash_c[:16]}...\")\r\n\r\n        success = self.chkpt_manager.load_state(exp_hash=hash_c)\r\n        self.assertTrue(success, \"State C should load successfully\")\r\n\r\n        # Verify training produces same results\r\n        model = ledgers.get_model()\r\n        dataloader = ledgers.get_dataloader()\r\n        optimizer = ledgers.get_optimizer()\r\n        criterion = ledgers.get_signal(name=\"train_mlt_loss/CE\")\r\n        criterion_bin = ledgers.get_signal(name=\"train_bin_loss/BCE\")\r\n\r\n        print(\"\\nTraining for 11 epochs to verify reproducibility...\")\r\n        pause_controller.resume()\r\n        loss_C_verify, uids_C_verify = self.train_epochs(model, dataloader, optimizer, criterion, num_epochs=self.config['training']['num_epochs'],\r\n            criterion_bin=criterion_bin)\r\n        pause_controller.pause()\r\n\r\n        # Check reproducibility with original loss and UIDs\r\n        # self.check_reproducibility(loss_c, loss_C_verify, uids_c, None, loss_tol=1e-1)\r\n\r\n        print(f\"\\nPart B: Modifying model from reloaded state C...\")\r\n\r\n        # Reload again to reset state\r\n        success = self.chkpt_manager.load_state(exp_hash=hash_c)\r\n\r\n        # Modify model\r\n        model = ledgers.get_model()\r\n        print(\"\\nModifying model architecture...\")\r\n        model.operate(0, {-2}, 1)  # Change conv1\r\n        model.operate(2, {-2}, 2)  # Change conv2\r\n\r\n        exp_hash_j, _, changed = self.chkpt_manager.update_experiment_hash()\r\n        print(f\"\\n[OK] New experiment hash J: {exp_hash_j[:16]}\")\r\n        self.assertIn('model', changed, \"Model should have changed\")\r\n\r\n        # Train with modified model\r\n        dataloader = ledgers.get_dataloader()\r\n        optimizer = ledgers.get_optimizer()\r\n\r\n        print(\"\\nTraining for 11 epochs with modified model...\")\r\n        pause_controller.resume()\r\n        loss_J, _ = self.train_epochs(model, dataloader, optimizer, criterion, num_epochs=self.config['training']['num_epochs'],\r\n            criterion_bin=criterion_bin)\r\n        pause_controller.pause()\r\n\r\n        # Store state\r\n        self.state['losses_j'] = sum(loss_J)/len(loss_J)\r\n        self.state['exp_hash_j'] = exp_hash_j\r\n\r\n        print(f\"\\n[OK] TEST 08 PASSED - Verified reproducibility and modified model\")\r\n\r\n    # ========================================================================\r\n    # Test: 09_reload_before_hp_change_verify_and_fix\r\n    # ========================================================================\r\n    def test_09_reload_before_hp_change_verify_and_modify(self):\r\n        \"\"\"Reload before HP change (state B), verify training, then fix HP, model, and data\"\"\"\r\n        print(f\"\\n{'='*80}\")\r\n        print(\"TEST 09: Reload Before HP Change - Verify and Fix Everything\")\r\n        print(f\"{'='*80}\\n\")\r\n\r\n        hash_b = self.state['exp_hash_b']  # Before HP change (after model change)\r\n        loss_b = self.state['losses_b']\r\n\r\n        print(f\"Part A: Reloading state B and verifying training reproducibility...\")\r\n        print(f\"Reloading state B: {hash_b[:16]}...\")\r\n\r\n        success = self.chkpt_manager.load_state(exp_hash=hash_b)\r\n        self.assertTrue(success, \"State B should load successfully\")\r\n\r\n        # Verify training produces same results\r\n        model = ledgers.get_model()\r\n        dataloader = ledgers.get_dataloader()\r\n        optimizer = ledgers.get_optimizer()\r\n        criterion = ledgers.get_signal(name=\"train_mlt_loss/CE\")\r\n        criterion_bin = ledgers.get_signal(name=\"train_bin_loss/BCE\")\r\n\r\n        print(\"\\nTraining for 11 epochs to verify reproducibility...\")\r\n        pause_controller.resume()\r\n        loss_B_verify, uids_B_verify = self.train_epochs(model, dataloader, optimizer, criterion, num_epochs=self.config['training']['num_epochs'],\r\n            criterion_bin=criterion_bin)\r\n        pause_controller.pause()\r\n\r\n        # Check reproducibility with original loss and UIDs\r\n        self.check_reproducibility(loss_b, loss_B_verify, self.state.get('uids_b'), None, loss_tol=1e-1)\r\n\r\n        print(f\"\\nPart B: Fixing HP, model, and data from reloaded state B...\")\r\n\r\n        # Reload again to reset state\r\n        success = self.chkpt_manager.load_state(exp_hash=hash_b)\r\n\r\n        # Fix HP\r\n        hp = ledgers.get_hyperparams()\r\n        hp['data']['train_loader']['batch_size'] = 7  # Change batch size\r\n\r\n        # Fix model\r\n        model = ledgers.get_model()\r\n        model.operate(0, {-3}, 1)  # Further modify conv1\r\n\r\n        # Fix data - discard 5 samples\r\n        dfm = ledgers.get_dataframe()\r\n        tagged_samples = random.sample(range(10), 2)\r\n        rows = []\r\n        for idx in tagged_samples:\r\n            uid = dfm.get_df_view().index[idx]\r\n            rows.append({\r\n                \"sample_id\": uid,\r\n                f\"{SampleStatsEx.TAG.value}:discard_fix\": True,\r\n                SampleStatsEx.DISCARDED.value: True\r\n            })\r\n        df_update = pd.DataFrame(rows).set_index(\"sample_id\")\r\n        dfm.upsert_df(df_update, origin='train_loader', force_flush=True)\r\n\r\n        exp_hash_k, _, changed = self.chkpt_manager.update_experiment_hash()\r\n        print(f\"\\n[OK] New experiment hash K: {exp_hash_k[:16]}\")\r\n        print(f\"[OK] Changed components: {changed}\")\r\n        self.assertIn('hp', changed, \"HP should have changed\")\r\n        self.assertIn('model', changed, \"Model should have changed\")\r\n        self.assertIn('data', changed, \"Data should have changed\")\r\n\r\n        # Train with all fixes\r\n        dataloader = ledgers.get_dataloader()\r\n        optimizer = ledgers.get_optimizer()\r\n        criterion = ledgers.get_signal(name=\"train_mlt_loss/CE\")\r\n        criterion_bin = ledgers.get_signal(name=\"train_bin_loss/BCE\")\r\n\r\n        print(\"\\nTraining for 11 epochs with all fixes...\")\r\n        pause_controller.resume()\r\n        loss_K, _ = self.train_epochs(model, dataloader, optimizer, criterion, num_epochs=self.config['training']['num_epochs'],\r\n            criterion_bin=criterion_bin)\r\n        pause_controller.pause()\r\n\r\n        # Store state\r\n        self.state['losses_k'] = sum(loss_K)/len(loss_K)\r\n        self.state['exp_hash_k'] = exp_hash_k\r\n\r\n        print(f\"\\n[OK] TEST 09 PASSED - Verified reproducibility and fixed everything\")\r\n\r\n    # ========================================================================\r\n    # Test: 10_reload_branch_j_verify_reproducibility\r\n    # ========================================================================\r\n    def test_10_reload_branch_j_verify_reproducibility(self):\r\n        \"\"\"Reload branch J (from test 08.b) and verify training reproducibility\"\"\"\r\n        print(f\"\\n{'='*80}\")\r\n        print(\"TEST 10: Reload Branch J - Verify Training Reproducibility\")\r\n        print(f\"{'='*80}\\n\")\r\n\r\n        hash_j = self.state['exp_hash_j']  # From test 08.b\r\n\r\n        print(f\"Reloading branch J: {hash_j[:16]}...\")\r\n\r\n        success = self.chkpt_manager.load_state(exp_hash=hash_j)\r\n        self.assertTrue(success, \"State J should load successfully\")\r\n\r\n        # Train again to verify reproducibility\r\n        model = ledgers.get_model()\r\n        dataloader = ledgers.get_dataloader()\r\n        optimizer = ledgers.get_optimizer()\r\n        criterion = ledgers.get_signal(name=\"train_mlt_loss/CE\")\r\n        criterion_bin = ledgers.get_signal(name=\"train_bin_loss/BCE\")\r\n\r\n        print(\"\\nTraining for 11 epochs to verify reproducibility...\")\r\n        pause_controller.resume()\r\n        loss_j_verify, _ = self.train_epochs(model, dataloader, optimizer, criterion, num_epochs=self.config['training']['num_epochs'],\r\n            criterion_bin=criterion_bin)\r\n        pause_controller.pause()\r\n\r\n        # Check reproducibility with original loss and UIDs\r\n        self.check_reproducibility(self.state['losses_j'], loss_j_verify, self.state.get('uids_b'), None, loss_tol=1e-1)\r\n\r\n        print(f\"\\n[OK] TEST 10 PASSED - Branch J training is reproducible\")\r\n\r\n    # ========================================================================\r\n    # Test: 11_restart_from_config_verify_reproducibility\r\n    # ========================================================================\r\n    def test_11_restart_from_scratch_to_hash_d_and_verify_reproducibility(self):\r\n        \"\"\"Test 11: Restart experiment from config - verify all components load to branch_j state\"\"\"\r\n        print(f\"\\n{'='*80}\")\r\n        print(\"TEST 11: Restart Experiment from Config - Verify Full Reproducibility\")\r\n        print(f\"{'='*80}\\n\")\r\n\r\n        # Reference variables\r\n        target_hash = self.state['exp_hash_d']  # Target is branch_d\r\n\r\n        print(f\"Simulating fresh restart: loading everything from config...\")\r\n        print(f\"Target state: {target_hash[:16]} (branch_d)\")\r\n\r\n        # Simulate fresh Python process: re-register everything from config\r\n        config_reloaded = self.config_cp\r\n\r\n        # Clear existing ledger entries\r\n        ledgers.clear_all()\r\n        print(\"[OK] Cleared existing ledger entries\")\r\n\r\n        # =================================================\r\n        # Automotically load components from existing chkpt\r\n        # =================================================\r\n        # First init a checkpoint manager with reloaded config\r\n        self.chkpt_manager = CheckpointManager(root_log_dir=self.config.get('root_log_dir'))\r\n        ledgers.register_checkpoint_manager(self.chkpt_manager)\r\n\r\n        # Re-register HP\r\n        register_in_ledger(config_reloaded, flag=\"hyperparameters\")\r\n        print(\"[OK] Hyperparameters re-registered\")\r\n\r\n        # Create fresh model\r\n        model_restarted = SimpleCNN(conv1_out=8, conv2_out=16)  # Match branch_d architecture\r\n        # # Model arch. and weights are updated at the init of model interface\r\n        model_restarted = register_in_ledger(model_restarted, flag=\"model\", device=DEVICE)\r\n\r\n        # Re-register dataloader\r\n        dataloader = register_in_ledger(\r\n            self.dataset,\r\n            flag=\"dataloader\",\r\n            loader_name='train_loader',\r\n            compute_hash=False,\r\n            is_training=True,\r\n            batch_size=self.config.get('data', {}).get('train_loader', {}).get('batch_size', 32),\r\n            shuffle=self.config.get('data', {}).get('train_loader', {}).get('shuffle', False)\r\n        )\r\n\r\n        # Optimizer and criterion\r\n        optimizer_restarted = th.optim.Adam(\r\n            model_restarted.parameters(),\r\n            lr=config_reloaded.get('optimizer', {}).get('lr', 0.001)\r\n        )\r\n        optimizer_restarted = register_in_ledger(optimizer_restarted, flag=\"optimizer\")\r\n        # # Create and register signal (criterion)\r\n        criterion = nn.CrossEntropyLoss(reduction='none')\r\n        criterion = register_in_ledger(\r\n            criterion,\r\n            flag=\"signal\",\r\n            name=\"train_mlt_loss/CE\",\r\n            log=True\r\n        )\r\n        criterion_bin = nn.BCEWithLogitsLoss(reduction='none')\r\n        criterion_bin = register_in_ledger(\r\n            criterion_bin,\r\n            flag=\"signal\",\r\n            name=\"train_bin_loss/BCE\",\r\n            log=True\r\n        )\r\n        print(\"[OK] Fresh registrations complete\")\r\n\r\n        # Get all hashes\r\n        all_hashes = self.chkpt_manager.get_all_hashes(sort_by='created')\r\n        print(f\"\\n[OK] Found {len(all_hashes)} experiment states:\")\r\n        for i, entry in enumerate(all_hashes):\r\n            print(f\"  {i+1}. {entry['hash'][:16]}... (created: {entry['created'][:19]})\")\r\n\r\n        # Reload state B (second state created)\r\n        hash_a_from_manifest = self.state['exp_hash_a']\r\n\r\n        print(f\"\\n[OK] Reloading state B: {hash_a_from_manifest[:16]}...\")\r\n\r\n        # Use new load_state method to load and apply checkpoint in-place\r\n        success = self.chkpt_manager.load_state(exp_hash=target_hash)\r\n        self.assertTrue(success, \"State should be loaded successfully\")\r\n\r\n        print(f\"[OK] Checkpoint loaded to reach target state {target_hash[:16]}\")\r\n        print(\"\\nTraining for 11 epochs to verify reproducibility...\")\r\n        pause_controller.resume()\r\n        _, _ = self.train_epochs(model_restarted, dataloader, optimizer_restarted, criterion, num_epochs=self.config['training']['num_epochs'],\r\n            criterion_bin=criterion_bin)\r\n        pause_controller.pause()\r\n\r\n        # Check reproducibility with original loss and UIDs\r\n        self.assertEqual(model_restarted.layers[-1].operation_age['FREEZE'], 1,\r\n                         \"Model architecture should match state in D\")\r\n        self.assertEqual(model_restarted.layers[-1].operation_age['RESET'], 1,\r\n                         \"Model architecture should match state in D\")\r\n        self.assertEqual(model_restarted.layers[0].out_neurons, 8,\r\n                         \"Model architecture should match state in D\")\r\n\r\n        # Not possible as data are generated randomly without reproducibility now\r\n        # self.check_reproducibility(loss_d_original, loss_d_verify, originals_uids, None, loss_tol=1e-1)\r\n\r\n    # ========================================================================\r\n    # Test: logger queue saved with weights\r\n    # ========================================================================\r\n    def test_logger_queue_saved_with_weights(self):\r\n        self.chkpt_manager.update_experiment_hash(force=False, dump_immediately=False)\r\n\r\n        snapshot_dir = Path(self.chkpt_manager.loggers_dir) / self.chkpt_manager.current_exp_hash\r\n        manifest_path = snapshot_dir / \"loggers.manifest.json\"\r\n        legacy_snapshot_path = snapshot_dir / \"loggers.json\"\r\n        self.assertTrue(\r\n            manifest_path.exists() or legacy_snapshot_path.exists(),\r\n            \"Logger snapshot should be saved with checkpoint\"\r\n        )\r\n\r\n        if manifest_path.exists():\r\n            with open(manifest_path, \"r\") as f:\r\n                manifest = json.load(f)\r\n            chunk_files = manifest.get(\"chunks\", [])\r\n            self.assertGreaterEqual(len(chunk_files), 1, \"Chunked logger snapshot should contain at least one chunk\")\r\n            self.assertTrue(\r\n                all((snapshot_dir / chunk_name).exists() for chunk_name in chunk_files),\r\n                \"All logger snapshot chunks referenced by manifest should exist\"\r\n            )\r\n            self.assertTrue(self.chkpt_manager.load_logger_snapshot(self.chkpt_manager.current_exp_hash))\r\n            lg = ledgers.get_logger('main')\r\n            loggers = {'main': {'signal_history': lg.get_signal_history() if hasattr(lg, 'get_signal_history') else []}}\r\n        else:\r\n            with open(legacy_snapshot_path, \"r\") as f:\r\n                snapshot = json.load(f)\r\n            loggers = snapshot.get(\"loggers\", {})\r\n\r\n        self.assertIn('main', loggers, \"Logger entry should be present\")\r\n        signals = loggers['main'].get(\"signal_history\", [])\r\n        self.assertGreaterEqual(len(signals), 1, \"Signal history should contain logged signals\")\r\n\r\n\r\nclass CheckpointStepAwareBehaviorTests(unittest.TestCase):\r\n    def test_model_hash_depends_on_init_step(self):\r\n        generator = ExperimentHashGenerator()\r\n        model = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 2))\r\n\r\n        hash_from_step_0 = generator.generate_hash(\r\n            model=model,\r\n            config={\"learning_rate\": 0.001},\r\n            data_state={},\r\n            model_init_step=0,\r\n        )\r\n        hash_from_step_52 = generator.generate_hash(\r\n            model=model,\r\n            config={\"learning_rate\": 0.001},\r\n            data_state={},\r\n            model_init_step=52,\r\n        )\r\n\r\n        self.assertNotEqual(\r\n            hash_from_step_0[8:16],\r\n            hash_from_step_52[8:16],\r\n            \"Model hash segment should change when model_init_step changes\",\r\n        )\r\n\r\n    def test_selects_closest_weights_checkpoint_for_target_step(self):\r\n        with tempfile.TemporaryDirectory(prefix=\"weights_step_select_\") as temp_dir:\r\n            manager = CheckpointManager(root_log_dir=temp_dir, load_model=False, load_config=False, load_data=False)\r\n            manager.current_exp_hash = \"12345678abcdef0199aabbcc\"\r\n\r\n            model = nn.Sequential(nn.Linear(3, 3))\r\n            manager.save_model_checkpoint(model=model, step=0, save_optimizer=False)\r\n            manager.save_model_checkpoint(model=model, step=5, save_optimizer=False)\r\n\r\n            picked = manager._select_weight_checkpoint_file(manager.current_exp_hash, target_step=3)\r\n            self.assertIsNotNone(picked, \"A checkpoint file should be selected\")\r\n            self.assertIn(\"_step_000005.pt\", picked.name)\r\n\r\n\r\nif __name__ == '__main__':\r\n    # Create test suite with explicit ordering\r\n    suite = unittest.TestSuite()\r\n\r\n    # Add tests in specific order\r\n    # # Initialize experiment\r\n    suite.addTest(CheckpointSystemTests('test_00_initialize_experiment'))\r\n    # # User Adventures training workflow\r\n    suite.addTest(CheckpointSystemTests('test_01_train_A'))\r\n    suite.addTest(CheckpointSystemTests('test_02_train_B_model_change'))\r\n    suite.addTest(CheckpointSystemTests('test_03_train_C_hyperparams_change'))\r\n    suite.addTest(CheckpointSystemTests('test_04_train_D_data_change'))\r\n    # # Reload and branching tests\r\n    suite.addTest(CheckpointSystemTests('test_05_train_E_reload_and_branch'))\r\n    suite.addTest(CheckpointSystemTests('test_06_reload_before_model_change'))\r\n    suite.addTest(CheckpointSystemTests('test_07_change_data_from_test06'))\r\n    # # Reload and check full reproducibility - Loss and UIDs\r\n    suite.addTest(CheckpointSystemTests('test_08_reload_before_data_change_verify_and_modify'))\r\n    suite.addTest(CheckpointSystemTests('test_09_reload_before_hp_change_verify_and_modify'))\r\n    suite.addTest(CheckpointSystemTests('test_10_reload_branch_j_verify_reproducibility'))\r\n    suite.addTest(CheckpointSystemTests('test_11_restart_from_scratch_to_hash_d_and_verify_reproducibility'))\r\n    # # Check that logger queue is saved and loaded\r\n    suite.addTest(CheckpointSystemTests('test_logger_queue_saved_with_weights'))\r\n    # # Step-aware hash/checkpoint behavior\r\n    suite.addTest(CheckpointStepAwareBehaviorTests('test_model_hash_depends_on_init_step'))\r\n    suite.addTest(CheckpointStepAwareBehaviorTests('test_selects_closest_weights_checkpoint_for_target_step'))\r\n\r\n    # Run the suite\r\n    runner = unittest.TextTestRunner(verbosity=2)\r\n    runner.run(suite)\r\n"
        }
    ]
}